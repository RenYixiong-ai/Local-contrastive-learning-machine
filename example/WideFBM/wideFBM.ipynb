{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "local_path = os.getcwd()\n",
    "# 将项目主目录路径添加到 Python 路径\n",
    "os.chdir(\"../../\")  # 使用相对路径将工作目录切换到 project 文件夹\n",
    "project_path = os.path.abspath(os.path.join(local_path, \"../../\"))\n",
    "sys.path.append(project_path)   #将模块查找路径切换\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from models import modelset\n",
    "from train.train import train_FBM\n",
    "from train.train import DFBM\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = [100]*10\n",
    "datatype = 'KMNIST'\n",
    "\n",
    "train_loader = get_dataloader(datatype, batch_size=64, train=True, class_counts=class_counts)\n",
    "#train_loader = get_dataloader(datatype, batch_size=64, train=True)\n",
    "test_loader = get_dataloader(datatype, batch_size=64, train=False)\n",
    "\n",
    "data_iter = iter(train_loader)\n",
    "images, labels = next(data_iter)\n",
    "batch, channel, large, _ = images.shape\n",
    "\n",
    "input_size = channel*large**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 标签修改\n",
    "将标签进行修改，只考虑其中选中的标签，将其它标签置0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def efface_label(selected_labels, labels, num_classes):\n",
    "    \"\"\"\n",
    "    根据指定的标签列表对输入标签进行处理，将指定的标签转换为 one-hot 编码，\n",
    "    其余未指定的标签对应的 one-hot 编码保持全零。\n",
    "\n",
    "    参数:\n",
    "        selected_labels (list or set): 一个包含指定标签的列表或集合。只有这些标签会被转换为 one-hot 编码，其余标签的 one-hot 编码保持全零。\n",
    "        labels (torch.Tensor): 输入的标签张量，形状为 (batch_size,)，每个值是一个整数，表示标签类别。\n",
    "        num_classes (int): 标签的类别总数，用于生成 one-hot 编码的长度。\n",
    "\n",
    "    返回:\n",
    "        torch.Tensor: 处理后的 one-hot 编码张量，形状为 (batch_size, num_classes)。其中，\n",
    "                      - 如果标签在 `selected_labels` 中，则生成对应的 one-hot 编码。\n",
    "                      - 如果标签不在 `selected_labels` 中，则对应的行全为 0。\n",
    "    \"\"\"\n",
    "    # 创建一个全零的 one-hot 编码张量，形状为 (batch_size, num_classes)\n",
    "    one_hot_labels = torch.zeros((labels.size(0), num_classes))\n",
    "\n",
    "    # 遍历每个输入标签\n",
    "    for i, label in enumerate(labels):\n",
    "        # 如果标签在 selected_labels 中，将其转换为 one-hot 编码\n",
    "        if label.item() in selected_labels:\n",
    "            one_hot_labels[i] = F.one_hot(label, num_classes=num_classes).float()\n",
    "\n",
    "    return one_hot_labels\n",
    "\n",
    "label = efface_label([1, 2], labels, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FBM树模型\n",
    "构建5个FBM，神经网络分类两组，并且将其它的类别分为一类。然后将这5个网络的输出结果构建为一个整体的向量，将这个向量通过一个全连接层进行分类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total valid combinations: 945\n",
      "Valid set 1: ((0, 1), (2, 3), (4, 5), (6, 7), (8, 9))\n",
      "Valid set 2: ((0, 1), (2, 3), (4, 5), (6, 8), (7, 9))\n",
      "Valid set 3: ((0, 1), (2, 3), (4, 5), (6, 9), (7, 8))\n",
      "Valid set 4: ((0, 1), (2, 3), (4, 6), (5, 7), (8, 9))\n",
      "Valid set 5: ((0, 1), (2, 3), (4, 6), (5, 8), (7, 9))\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    将所有可能的两两组合情况进行枚举\n",
    "'''\n",
    "\n",
    "# 构造所有可能的两两排列组合\n",
    "import itertools\n",
    "\n",
    "# 生成 0-9 的所有两两组合（不考虑顺序）\n",
    "all_combinations = list(itertools.combinations(range(10), 2))\n",
    "\n",
    "# 函数：检查是否覆盖所有数字 0-9\n",
    "def covers_all_digits(groups):\n",
    "    # 获取所有组合中的数字\n",
    "    digits = set()\n",
    "    for group in groups:\n",
    "        digits.update(group)\n",
    "    # 检查是否覆盖 0-9\n",
    "    return digits == set(range(10))\n",
    "\n",
    "# 从所有组合中选择 5 组\n",
    "valid_groups = []\n",
    "for groups in itertools.combinations(all_combinations, 5):\n",
    "    if covers_all_digits(groups):\n",
    "        valid_groups.append(groups)\n",
    "\n",
    "# 输出结果\n",
    "print(f\"Total valid combinations: {len(valid_groups)}\")\n",
    "for i, groups in enumerate(valid_groups[:5]):  # 示例输出前 5 种可能情况\n",
    "    print(f\"Valid set {i + 1}: {groups}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 0.5664\n",
      "Epoch [2/50], Loss: 0.5664\n",
      "Epoch [3/50], Loss: 0.5664\n",
      "Epoch [4/50], Loss: 0.5664\n",
      "Epoch [5/50], Loss: 0.5664\n",
      "Epoch [6/50], Loss: 0.5664\n",
      "Epoch [7/50], Loss: 0.5664\n",
      "Epoch [8/50], Loss: 0.5664\n",
      "Epoch [9/50], Loss: 0.5664\n",
      "Epoch [10/50], Loss: 0.5664\n",
      "Epoch [11/50], Loss: 0.6076\n",
      "Epoch [12/50], Loss: 0.6076\n",
      "Epoch [13/50], Loss: 0.6076\n",
      "Epoch [14/50], Loss: 0.6076\n",
      "Epoch [15/50], Loss: 0.6076\n",
      "Epoch [16/50], Loss: 0.6076\n",
      "Epoch [17/50], Loss: 0.6076\n",
      "Epoch [18/50], Loss: 0.6076\n",
      "Epoch [19/50], Loss: 0.6076\n",
      "Epoch [20/50], Loss: 0.6076\n",
      "Epoch [21/50], Loss: 0.6269\n",
      "Epoch [22/50], Loss: 0.6269\n",
      "Epoch [23/50], Loss: 0.6269\n",
      "Epoch [24/50], Loss: 0.6269\n",
      "Epoch [25/50], Loss: 0.6269\n",
      "Epoch [26/50], Loss: 0.6269\n",
      "Epoch [27/50], Loss: 0.6269\n",
      "Epoch [28/50], Loss: 0.6269\n",
      "Epoch [29/50], Loss: 0.6269\n",
      "Epoch [30/50], Loss: 0.6269\n",
      "Epoch [31/50], Loss: 0.6282\n",
      "Epoch [32/50], Loss: 0.6282\n",
      "Epoch [33/50], Loss: 0.6282\n",
      "Epoch [34/50], Loss: 0.6282\n",
      "Epoch [35/50], Loss: 0.6282\n",
      "Epoch [36/50], Loss: 0.6282\n",
      "Epoch [37/50], Loss: 0.6282\n",
      "Epoch [38/50], Loss: 0.6282\n",
      "Epoch [39/50], Loss: 0.6282\n",
      "Epoch [40/50], Loss: 0.6282\n",
      "Epoch [41/50], Loss: 0.6283\n",
      "Epoch [42/50], Loss: 0.6283\n",
      "Epoch [43/50], Loss: 0.6283\n",
      "Epoch [44/50], Loss: 0.6283\n",
      "Epoch [45/50], Loss: 0.6283\n",
      "Epoch [46/50], Loss: 0.6283\n",
      "Epoch [47/50], Loss: 0.6283\n",
      "Epoch [48/50], Loss: 0.6283\n",
      "Epoch [49/50], Loss: 0.6283\n",
      "Epoch [50/50], Loss: 0.6283\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    一次完整的训练测试过程\n",
    "'''\n",
    "\n",
    "\n",
    "NN_outsize_list = [100, 100, 100, 100, 100]\n",
    "df_list = [0.45, 0.45, 0.45, 0.45, 0.45] \n",
    "alpha_list = [1.0, 1.0, 1.0, 1.0, 1.0]\n",
    "sel_label = valid_groups[34]\n",
    "\n",
    "from models import modelset as models\n",
    "from loss.loss import FBMLoss\n",
    "from loss.loss import test_accuracy\n",
    "\n",
    "par_model = []\n",
    "for NN_outsize, df, alpha, sel_label in zip(NN_outsize_list, df_list, alpha_list, sel_label):\n",
    "    model = modelset.FBMLayer(input_size, NN_outsize).to(device)\n",
    "    criterion = FBMLoss(NN_outsize, 0.01, df=df, alpha=alpha, if_onehot=True)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    model.train()\n",
    "    # 训练模型\n",
    "    for epoch in range(30):\n",
    "        for images, labels in train_loader:\n",
    "            # 将图像和标签移动到 GPU 上\n",
    "            images = images.view(-1, input_size).to(device)  # 展平图像并转移到 GPU\n",
    "            labels = efface_label([1, 2], labels, 10)\n",
    "            labels = labels.to(device)  # 标签移动到 GPU\n",
    "            #labels_one_hot = F.one_hot(labels, num_classes=num_classes).float()\n",
    "            \n",
    "            # 前向传播\n",
    "            outputs = model(images)\n",
    "            #loss = criterion(outputs, labels_one_hot, model.linear.weight)\n",
    "            loss = criterion(outputs, labels, model.linear.weight)\n",
    "            \n",
    "            # 反向传播和优化\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "    par_model.append(model)\n",
    "\n",
    "par_model = models.ParallelNetworks(par_model).eval()\n",
    "\n",
    "from models import MLP\n",
    "# 定义损失函数和优化器\n",
    "modelout = MLP(sum(NN_outsize_list), 10).to(device)\n",
    "criterion2 = nn.CrossEntropyLoss()  # 使用交叉熵损失\n",
    "optimizer = optim.Adam(modelout.parameters(), lr=0.01)  # 使用随机梯度下降优化器\n",
    "\n",
    "modelout.train()\n",
    "# 训练模型\n",
    "max_accury = 0.0\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    for images, labels in train_loader:\n",
    "        # 将图像展平为一维向量，并将标签进行 one-hot 编码\n",
    "        images = images.view(-1, input_size).to(device)  # 展平图像\n",
    "        labels_one_hot = F.one_hot(labels, num_classes=10).float().to(device)  # 将标签转换为 one-hot 编码\n",
    "\n",
    "        # 前向传播\n",
    "        with torch.no_grad():\n",
    "            deal_images = par_model(images)\n",
    "            #deal_images = model6(deal_images)\n",
    "\n",
    "        outputs = modelout(deal_images)\n",
    "\n",
    "        # 计算损失\n",
    "        loss = criterion2(outputs, labels_one_hot)\n",
    "\n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        total_model = models.ModelPipeline()\n",
    "        total_model.add_model(par_model)\n",
    "        total_model.add_model(modelout)\n",
    "        max_accury = max(test_accuracy(total_model, test_loader, device), max_accury)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {max_accury:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelNetworks(nn.Module):\n",
    "    def __init__(self, networks):\n",
    "        \"\"\"\n",
    "        初始化多网络模块。\n",
    "\n",
    "        参数:\n",
    "            networks (list): 一个包含多个神经网络的列表，每个网络将接收相同的输入。\n",
    "        \"\"\"\n",
    "        super(ParallelNetworks, self).__init__()\n",
    "        self.networks = nn.ModuleList(networks)  # 将网络列表包装成 nn.ModuleList\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        前向传播，同时运行所有网络，并将结果拼接。\n",
    "\n",
    "        参数:\n",
    "            x (torch.Tensor): 输入张量，形状为 [batch_size, ...]。\n",
    "\n",
    "        返回:\n",
    "            torch.Tensor: 拼接后的输出张量。\n",
    "        \"\"\"\n",
    "        # 同时运行多个网络，并将它们的输出收集到一个列表中\n",
    "        outputs = [network(x) for network in self.networks]\n",
    "        # 沿着最后一维 (feature 维度) 拼接所有网络的输出\n",
    "        return torch.cat(outputs, dim=1)\n",
    "\n",
    "model = ParallelNetworks([model1, model2, model3, model4, model5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6441"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''用于测试正确性的函数'''\n",
    "\n",
    "def test_accuracy(model, test_loader):\n",
    "    # 准确率计数\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # 得到inout_size\n",
    "    data_iter = iter(train_loader)\n",
    "    images, labels = next(data_iter)\n",
    "    batch, channel, large, _ = images.shape\n",
    "    input_size = channel*large**2\n",
    "\n",
    "    # 禁用梯度计算，加速测试过程\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            # 将数据加载到 GPU\n",
    "            images = images.view(-1, input_size).to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # 前向传播\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # 获取预测结果\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            # 更新计数\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # 计算准确率\n",
    "    accuracy = 1.0 * correct / total\n",
    "    #print(f'Accuracy on the test dataset: {accuracy:.2f}%') \n",
    "\n",
    "    return accuracy\n",
    "\n",
    "from models import modelset as models\n",
    "\n",
    "total_model = models.ModelPipeline()\n",
    "total_model.add_model(models.ParallelNetworks([model1, model2, model3, model4, model5]))\n",
    "total_model.add_model(modelout)\n",
    "test_accuracy(total_model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "normal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
