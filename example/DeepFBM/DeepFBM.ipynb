{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "local_path = os.getcwd()\n",
    "# 将项目主目录路径添加到 Python 路径\n",
    "os.chdir(\"../../\")  # 使用相对路径将工作目录切换到 project 文件夹\n",
    "project_path = os.path.abspath(os.path.join(local_path, \"../../\"))\n",
    "sys.path.append(project_path)   #将模块查找路径切换\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from models import modelset\n",
    "from train.train import train_FBM\n",
    "from train.train import DFBM\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = [100]*10\n",
    "datatype = 'KMNIST'\n",
    "\n",
    "train_loader = get_dataloader(datatype, batch_size=64, train=True, class_counts=class_counts)\n",
    "test_loader = get_dataloader(datatype, batch_size=64, train=False)\n",
    "\n",
    "data_iter = iter(train_loader)\n",
    "images, labels = next(data_iter)\n",
    "batch, channel, large, _ = images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import csv\n",
    "csv_file_path = os.path.join(local_path, 'optuna_results.csv')\n",
    "\n",
    "# 如果文件不存在，则创建文件并写入标题行\n",
    "if not os.path.exists(csv_file_path):\n",
    "    with open(csv_file_path, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['trial_number', 'output_size_list', 'd_f_list', 'alpha_list', 'accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 逐层训练的FBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    进行一次推理的过程\n",
    "'''\n",
    "\n",
    "# 定义超参数\n",
    "images_size = channel * large**2 # MNIST图像大小是28x28\n",
    "num_classes = 10      # MNIST有10个类别\n",
    "learning_rate = 0.01\n",
    "lam = 0.01\n",
    "num_epochs = 20\n",
    "batch_size = 64\n",
    "\n",
    "output_size_list = [1000, 1000, 1000, 1000, 1000]\n",
    "d_f_list = [0.0, 0.04, 0.1, 0.3, 0.5]\n",
    "alpha_list = [0.0, 100, 5.0, 2.0, 1.0]\n",
    "deep = 1\n",
    "\n",
    "pipeline, deal_train_loader =DFBM(train_loader, output_size_list, d_f_list, alpha_list, device)\n",
    "\n",
    "output_size = output_size_list[-1]\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "model2 = modelset.MLP(output_size, num_classes).to(device)\n",
    "criterion2 = nn.CrossEntropyLoss()  # 使用交叉熵损失\n",
    "optimizer = optim.Adam(model2.parameters(), lr=learning_rate)  # 使用随机梯度下降优化器\n",
    "\n",
    "model2.train()\n",
    "# 训练模型\n",
    "epochs = 30\n",
    "for epoch in range(epochs):\n",
    "    for images, labels in deal_train_loader:\n",
    "        # 将图像展平为一维向量，并将标签进行 one-hot 编码\n",
    "        images = images.to(device)\n",
    "        labels_one_hot = F.one_hot(labels, num_classes=num_classes).float().to(device)  # 将标签转换为 one-hot 编码\n",
    "\n",
    "        # 前向传播\n",
    "        outputs = model2(images)\n",
    "\n",
    "        # 计算损失\n",
    "        loss = criterion2(outputs, labels_one_hot)\n",
    "\n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    #print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "# 设置模型为评估模式\n",
    "pipeline.add_model(model2)\n",
    "pipeline.eval()\n",
    "\n",
    "# 准确率计数\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# 禁用梯度计算，加速测试过程\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        # 将数据加载到 GPU\n",
    "        images = images.view(-1, images_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # 前向传播\n",
    "        outputs = pipeline(images)\n",
    "        \n",
    "        # 获取预测结果\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        # 更新计数\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "# 计算准确率\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy on the test dataset: {accuracy:.2f}%')\n",
    "#return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置函数用于搜索\n",
    "\n",
    "def objective(trial):\n",
    "    # 定义超参数\n",
    "    images_size = channel * large**2 # MNIST图像大小是28x28\n",
    "    num_classes = 10      # MNIST有10个类别\n",
    "    learning_rate = 0.01\n",
    "    lam = 0.01\n",
    "    num_epochs = 20\n",
    "    batch_size = 64\n",
    "\n",
    "    # 使用 suggest_int 和 suggest_float 创建参数数组\n",
    "    output_size_list = [trial.suggest_int(f'output_size_{i}', 500, 2000) for i in range(3)] \n",
    "    d_f_list = [trial.suggest_float(f'd_f_{i}', 0.0, 1.0) for i in range(3)]\n",
    "    alpha_list = [trial.suggest_float(f'alpha_{i}', 0.1, 2.0) for i in range(3)]\n",
    "\n",
    "    pipeline, deal_train_loader =DFBM(train_loader, output_size_list, d_f_list, alpha_list, device)\n",
    "\n",
    "    output_size = output_size_list[-1]\n",
    "\n",
    "    # 定义损失函数和优化器\n",
    "    model2 = modelset.MLP(output_size, num_classes).to(device)\n",
    "    criterion2 = nn.CrossEntropyLoss()  # 使用交叉熵损失\n",
    "    optimizer = optim.Adam(model2.parameters(), lr=learning_rate)  # 使用随机梯度下降优化器\n",
    "\n",
    "    model2.train()\n",
    "    # 训练模型\n",
    "    epochs = 30\n",
    "    for epoch in range(epochs):\n",
    "        for images, labels in deal_train_loader:\n",
    "            # 将图像展平为一维向量，并将标签进行 one-hot 编码\n",
    "            images = images.to(device)\n",
    "            labels_one_hot = F.one_hot(labels, num_classes=num_classes).float().to(device)  # 将标签转换为 one-hot 编码\n",
    "\n",
    "            # 前向传播\n",
    "            outputs = model2(images)\n",
    "\n",
    "            # 计算损失\n",
    "            loss = criterion2(outputs, labels_one_hot)\n",
    "\n",
    "            # 反向传播和优化\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # 设置模型为评估模式\n",
    "    pipeline.add_model(model2)\n",
    "    pipeline.eval()\n",
    "\n",
    "    # 准确率计数\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # 禁用梯度计算，加速测试过程\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            # 将数据加载到 GPU\n",
    "            images = images.view(-1, images_size).to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # 前向传播\n",
    "            outputs = pipeline(images)\n",
    "            \n",
    "            # 获取预测结果\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            # 更新计数\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # 计算准确率\n",
    "    accuracy = 1.0 * correct / total\n",
    "\n",
    "    with open(csv_file_path, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([trial.number, output_size_list, d_f_list, alpha_list, accuracy])\n",
    "\n",
    "\n",
    "    return 1 - accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "# 创建 Optuna study 并优化\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=3)  # 进行 100 次优化搜索\n",
    "\n",
    "# 输出最佳参数\n",
    "print(\"Best parameters:\", study.best_params)\n",
    "print(\"Best validation loss:\", study.best_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 整体训练FBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    进行一次推理的过程\n",
    "'''\n",
    "from train.train import pre_DFBM\n",
    "# 定义超参数\n",
    "images_size = channel * large**2 # MNIST图像大小是28x28\n",
    "num_classes = 10      # MNIST有10个类别\n",
    "learning_rate = 0.01\n",
    "lam = 0.01\n",
    "num_epochs = 20\n",
    "batch_size = 64\n",
    "\n",
    "output_size_list = [1000, 1000, 1000, 1000, 1000]\n",
    "d_f_list = [0.0, 0.04, 0.1, 0.3, 0.5]\n",
    "alpha_list = [0.0, 100, 5.0, 2.0, 1.0]\n",
    "deep = 1\n",
    "\n",
    "pipeline, deal_train_loader =pre_DFBM(train_loader, output_size_list, d_f_list, alpha_list, device, train_MLP=True)\n",
    "\n",
    "output_size = output_size_list[-1]\n",
    "\n",
    "pipeline.eval()\n",
    "\n",
    "# 准确率计数\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# 禁用梯度计算，加速测试过程\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        # 将数据加载到 GPU\n",
    "        images = images.view(-1, images_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # 前向传播\n",
    "        outputs = pipeline(images)\n",
    "        \n",
    "        # 获取预测结果\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        # 更新计数\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "# 计算准确率\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy on the test dataset: {accuracy:.2f}%')\n",
    "#return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置函数用于搜索\n",
    "\n",
    "def objective(trial):\n",
    "    # 定义超参数\n",
    "    images_size = channel * large**2 # MNIST图像大小是28x28\n",
    "    num_classes = 10      # MNIST有10个类别\n",
    "    learning_rate = 0.01\n",
    "    lam = 0.01\n",
    "    num_epochs = 20\n",
    "    batch_size = 64\n",
    "\n",
    "    # 使用 suggest_int 和 suggest_float 创建参数数组\n",
    "    output_size_list = [trial.suggest_int(f'output_size_{i}', 500, 2000) for i in range(3)] \n",
    "    d_f_list = [trial.suggest_float(f'd_f_{i}', 0.0, 1.0) for i in range(3)]\n",
    "    alpha_list = [trial.suggest_float(f'alpha_{i}', 0.1, 2.0) for i in range(3)]\n",
    "\n",
    "    pipeline, deal_train_loader = pre_DFBM(train_loader, output_size_list, d_f_list, alpha_list, device, train_MLP=True)\n",
    "\n",
    "    # 设置模型为评估模式\n",
    "    pipeline.eval()\n",
    "\n",
    "    # 准确率计数\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # 禁用梯度计算，加速测试过程\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            # 将数据加载到 GPU\n",
    "            images = images.view(-1, images_size).to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # 前向传播\n",
    "            outputs = pipeline(images)\n",
    "            \n",
    "            # 获取预测结果\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            # 更新计数\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # 计算准确率\n",
    "    accuracy = 1.0 * correct / total\n",
    "\n",
    "    with open(csv_file_path, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([trial.number, output_size_list, d_f_list, alpha_list, accuracy])\n",
    "\n",
    "\n",
    "    return 1 - accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-15 01:04:22,245] A new study created in memory with name: no-name-cf90de25-08d7-4474-90c6-908804983302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "Epoch [1/30], Loss: ['0.2272', '0.3214', '0.3457', '2.6200']\n",
      "Epoch [2/30], Loss: ['0.2229', '0.2810', '0.3201', '1.6832']\n",
      "Epoch [3/30], Loss: ['0.2215', '0.2744', '0.3019', '1.5062']\n",
      "Epoch [4/30], Loss: ['0.1989', '0.2380', '0.2313', '1.3279']\n",
      "Epoch [5/30], Loss: ['0.2006', '0.2352', '0.2301', '1.6601']\n",
      "Epoch [6/30], Loss: ['0.2038', '0.2447', '0.2495', '1.3525']\n",
      "Epoch [7/30], Loss: ['0.1997', '0.2394', '0.2375', '0.9479']\n",
      "Epoch [8/30], Loss: ['0.1971', '0.2336', '0.2342', '1.1970']\n",
      "Epoch [9/30], Loss: ['0.2064', '0.2362', '0.2387', '1.2642']\n",
      "Epoch [10/30], Loss: ['0.1745', '0.2136', '0.2035', '0.8374']\n",
      "Epoch [11/30], Loss: ['0.1965', '0.2335', '0.2250', '0.8966']\n",
      "Epoch [12/30], Loss: ['0.1869', '0.2309', '0.2144', '0.7979']\n",
      "Epoch [13/30], Loss: ['0.1860', '0.2219', '0.2126', '0.9876']\n",
      "Epoch [14/30], Loss: ['0.1884', '0.2218', '0.2151', '1.1715']\n",
      "Epoch [15/30], Loss: ['0.1855', '0.2258', '0.2179', '1.2083']\n",
      "Epoch [16/30], Loss: ['0.1993', '0.2363', '0.2175', '1.2846']\n",
      "Epoch [17/30], Loss: ['0.1846', '0.2186', '0.2013', '0.8136']\n",
      "Epoch [18/30], Loss: ['0.1872', '0.2204', '0.2110', '1.0314']\n",
      "Epoch [19/30], Loss: ['0.1884', '0.2066', '0.1882', '0.8711']\n",
      "Epoch [20/30], Loss: ['0.1886', '0.2324', '0.2007', '0.7494']\n",
      "Epoch [21/30], Loss: ['0.1793', '0.2105', '0.1982', '0.7335']\n",
      "Epoch [22/30], Loss: ['0.1814', '0.2067', '0.1978', '1.0484']\n",
      "Epoch [23/30], Loss: ['0.1703', '0.2082', '0.1888', '1.0125']\n",
      "Epoch [24/30], Loss: ['0.1956', '0.2308', '0.2133', '1.2517']\n",
      "Epoch [25/30], Loss: ['0.1786', '0.2057', '0.1852', '0.5061']\n",
      "Epoch [26/30], Loss: ['0.1745', '0.1969', '0.1851', '1.0774']\n",
      "Epoch [27/30], Loss: ['0.1771', '0.2082', '0.1913', '0.9147']\n",
      "Epoch [28/30], Loss: ['0.1816', '0.2077', '0.1912', '1.0579']\n",
      "Epoch [29/30], Loss: ['0.1718', '0.1961', '0.1944', '0.6319']\n",
      "Epoch [30/30], Loss: ['0.1895', '0.2223', '0.2026', '0.9757']\n",
      "Processed data shape: torch.Size([1000, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-15 01:04:33,118] Trial 0 finished with value: 0.4679 and parameters: {'output_size_0': 680, 'output_size_1': 1667, 'output_size_2': 546, 'd_f_0': 0.13905803011331241, 'd_f_1': 0.7427427574543171, 'd_f_2': 0.5994859049072435, 'alpha_0': 1.2801852868072683, 'alpha_1': 0.325585086243613, 'alpha_2': 0.9831804569199781}. Best is trial 0 with value: 0.4679.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "Epoch [1/30], Loss: ['0.0720', '0.0789', '0.0894', '1.7814']\n",
      "Epoch [2/30], Loss: ['0.0725', '0.0597', '0.0594', '0.7251']\n",
      "Epoch [3/30], Loss: ['0.0793', '0.0656', '0.0692', '0.9435']\n",
      "Epoch [4/30], Loss: ['0.0753', '0.0549', '0.0547', '1.6481']\n",
      "Epoch [5/30], Loss: ['0.0708', '0.0575', '0.0559', '0.8399']\n",
      "Epoch [6/30], Loss: ['0.0729', '0.0546', '0.0586', '1.0720']\n",
      "Epoch [7/30], Loss: ['0.0718', '0.0538', '0.0588', '0.7437']\n",
      "Epoch [8/30], Loss: ['0.0614', '0.0466', '0.0457', '0.3612']\n",
      "Epoch [9/30], Loss: ['0.0610', '0.0476', '0.0494', '0.5034']\n",
      "Epoch [10/30], Loss: ['0.0579', '0.0449', '0.0455', '0.4066']\n",
      "Epoch [11/30], Loss: ['0.0625', '0.0497', '0.0544', '0.4451']\n",
      "Epoch [12/30], Loss: ['0.0578', '0.0451', '0.0456', '0.2523']\n",
      "Epoch [13/30], Loss: ['0.0637', '0.0515', '0.0557', '1.0345']\n",
      "Epoch [14/30], Loss: ['0.0570', '0.0498', '0.0561', '0.6796']\n",
      "Epoch [15/30], Loss: ['0.0587', '0.0481', '0.0487', '0.4368']\n",
      "Epoch [16/30], Loss: ['0.0653', '0.0482', '0.0543', '0.8809']\n",
      "Epoch [17/30], Loss: ['0.0579', '0.0502', '0.0539', '0.7283']\n",
      "Epoch [18/30], Loss: ['0.0608', '0.0464', '0.0477', '0.4561']\n",
      "Epoch [19/30], Loss: ['0.0599', '0.0423', '0.0369', '0.0507']\n",
      "Epoch [20/30], Loss: ['0.0567', '0.0442', '0.0472', '0.4688']\n",
      "Epoch [21/30], Loss: ['0.0577', '0.0418', '0.0456', '0.6451']\n",
      "Epoch [22/30], Loss: ['0.0549', '0.0405', '0.0408', '0.3616']\n",
      "Epoch [23/30], Loss: ['0.0531', '0.0349', '0.0341', '0.3299']\n",
      "Epoch [24/30], Loss: ['0.0622', '0.0426', '0.0472', '0.3638']\n",
      "Epoch [25/30], Loss: ['0.0601', '0.0425', '0.0442', '0.3590']\n",
      "Epoch [26/30], Loss: ['0.0644', '0.0465', '0.0494', '0.5544']\n",
      "Epoch [27/30], Loss: ['0.0553', '0.0428', '0.0435', '0.3322']\n",
      "Epoch [28/30], Loss: ['0.0583', '0.0388', '0.0392', '0.2019']\n",
      "Epoch [29/30], Loss: ['0.0530', '0.0352', '0.0352', '0.2384']\n",
      "Epoch [30/30], Loss: ['0.0573', '0.0398', '0.0411', '0.3241']\n",
      "Processed data shape: torch.Size([1000, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-15 01:04:42,645] Trial 1 finished with value: 0.4003 and parameters: {'output_size_0': 735, 'output_size_1': 663, 'output_size_2': 943, 'd_f_0': 0.8153903692088532, 'd_f_1': 0.009028437027154279, 'd_f_2': 0.11333105433957158, 'alpha_0': 1.8123843235799697, 'alpha_1': 0.7579446857438504, 'alpha_2': 1.232740717658952}. Best is trial 1 with value: 0.4003.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "Epoch [1/30], Loss: ['0.6886', '0.8162', '1.1380', '5.8990']\n",
      "Epoch [2/30], Loss: ['0.6526', '0.8281', '1.1749', '2.5865']\n",
      "Epoch [3/30], Loss: ['0.7182', '0.7059', '1.1428', '2.2419']\n",
      "Epoch [4/30], Loss: ['0.6571', '0.8937', '1.0580', '2.0924']\n",
      "Epoch [5/30], Loss: ['0.6448', '0.8119', '0.9811', '2.7198']\n",
      "Epoch [6/30], Loss: ['0.6748', '0.8597', '1.0433', '3.0347']\n",
      "Epoch [7/30], Loss: ['0.6341', '0.8165', '1.0602', '2.0728']\n",
      "Epoch [8/30], Loss: ['0.6617', '0.8746', '1.1070', '2.4560']\n",
      "Epoch [9/30], Loss: ['0.6454', '0.7806', '0.9012', '1.7785']\n",
      "Epoch [10/30], Loss: ['0.6700', '0.8484', '1.0350', '2.2765']\n",
      "Epoch [11/30], Loss: ['0.6466', '0.8737', '1.0527', '2.2097']\n",
      "Epoch [12/30], Loss: ['0.6428', '0.8618', '0.9749', '2.0495']\n",
      "Epoch [13/30], Loss: ['0.6328', '0.8559', '1.0129', '1.8174']\n",
      "Epoch [14/30], Loss: ['0.6326', '0.8191', '0.9275', '2.2583']\n",
      "Epoch [15/30], Loss: ['0.6384', '0.8182', '0.9841', '2.2664']\n",
      "Epoch [16/30], Loss: ['0.6431', '0.8433', '0.9524', '2.1316']\n",
      "Epoch [17/30], Loss: ['0.6283', '0.8285', '0.9206', '2.0482']\n",
      "Epoch [18/30], Loss: ['0.6499', '0.8541', '0.8109', '3.3351']\n",
      "Epoch [19/30], Loss: ['0.6605', '0.8902', '0.9679', '2.3058']\n",
      "Epoch [20/30], Loss: ['0.6454', '0.7982', '0.8180', '1.8059']\n",
      "Epoch [21/30], Loss: ['0.6485', '0.7858', '0.9289', '1.9535']\n",
      "Epoch [22/30], Loss: ['0.6228', '0.8243', '0.8773', '1.8022']\n",
      "Epoch [23/30], Loss: ['0.6244', '0.7941', '0.8897', '2.0108']\n",
      "Epoch [24/30], Loss: ['0.5998', '0.6886', '0.8111', '1.6409']\n",
      "Epoch [25/30], Loss: ['0.6304', '0.8011', '0.9299', '1.4941']\n",
      "Epoch [26/30], Loss: ['0.6495', '0.7590', '0.7975', '2.8267']\n",
      "Epoch [27/30], Loss: ['0.6499', '0.7313', '0.8493', '2.0630']\n",
      "Epoch [28/30], Loss: ['0.6439', '0.7723', '0.8245', '1.8956']\n",
      "Epoch [29/30], Loss: ['0.6510', '0.7912', '0.8305', '1.4302']\n",
      "Epoch [30/30], Loss: ['0.6829', '0.9291', '0.9411', '1.8979']\n",
      "Processed data shape: torch.Size([1000, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-15 01:04:52,566] Trial 2 finished with value: 0.7289 and parameters: {'output_size_0': 1715, 'output_size_1': 935, 'output_size_2': 1533, 'd_f_0': 0.10653502124273984, 'd_f_1': 0.4281337758966294, 'd_f_2': 0.9236820373961723, 'alpha_0': 1.3621101624905176, 'alpha_1': 1.0496017411257144, 'alpha_2': 1.4846732647892475}. Best is trial 1 with value: 0.4003.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'output_size_0': 735, 'output_size_1': 663, 'output_size_2': 943, 'd_f_0': 0.8153903692088532, 'd_f_1': 0.009028437027154279, 'd_f_2': 0.11333105433957158, 'alpha_0': 1.8123843235799697, 'alpha_1': 0.7579446857438504, 'alpha_2': 1.232740717658952}\n",
      "Best validation loss: 0.4003\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "# 创建 Optuna study 并优化\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=3)  # 进行 100 次优化搜索\n",
    "\n",
    "# 输出最佳参数\n",
    "print(\"Best parameters:\", study.best_params)\n",
    "print(\"Best validation loss:\", study.best_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "normal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
