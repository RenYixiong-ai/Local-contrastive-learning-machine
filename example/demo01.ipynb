{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "local_path = os.getcwd()\n",
    "# 设置工作目录为项目的主目录\n",
    "os.chdir(os.path.join(local_path, \"../\"))  # 使用相对路径将工作目录切换到 project 文件夹\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "project_path = os.path.abspath(os.path.join(local_path, \"../../\"))\n",
    "sys.path.append(project_path)   #将模块查找路径切换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# 检查是否有可用的GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 设置随机种子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def set_seed(seed):\n",
    "    # 设置 Python 内置 random 库的随机种子\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # 设置 NumPy 的随机种子\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # 设置 PyTorch 的随机种子\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # 如果使用 GPU 进行训练，则需设置以下两个以保证完全的可重复性\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  # 如果使用多块 GPU\n",
    "    \n",
    "    # 设置 CuDNN 后端以确保结果一致性\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# 使用示例\n",
    "set_seed(42)  # 42 是一个示例种子数，您可以根据需求更改"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# 定义数据变换\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# 下载并加载训练数据集\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# 下载并加载测试数据集\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# 检查数据加载是否成功\n",
    "data_iter = iter(train_loader)\n",
    "images, labels = next(data_iter)\n",
    "print(f\"Images batch shape: {images.shape}\")\n",
    "print(f\"Labels batch shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "\n",
    "# 定义数据变换\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# 加载完整的 MNIST 数据集\n",
    "full_train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "# 下载并加载测试数据集\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# 初始化每个类别的索引列表\n",
    "class_indices = {i: [] for i in range(10)}\n",
    "\n",
    "# 遍历数据集，记录每个类别的索引\n",
    "for idx, (_, label) in enumerate(full_train_dataset):\n",
    "    if len(class_indices[label]) < 100:  # 每个类别最多选取 100 个样本\n",
    "        class_indices[label].append(idx)\n",
    "    if all(len(class_indices[i]) == 100 for i in range(10)):\n",
    "        break  # 当每个类别都有 100 个样本时停止\n",
    "\n",
    "# 合并所有类别的索引，形成所需的子集\n",
    "selected_indices = [idx for indices in class_indices.values() for idx in indices]\n",
    "\n",
    "# 创建自定义的训练数据集\n",
    "small_train_dataset = Subset(full_train_dataset, selected_indices)\n",
    "train_loader = DataLoader(small_train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# 检查数据加载是否成功\n",
    "data_iter = iter(train_loader)\n",
    "images, labels = next(data_iter)\n",
    "print(f\"Images batch shape: {images.shape}\")\n",
    "print(f\"Labels batch shape: {labels.shape}\")\n",
    "print(f\"Sample labels: {labels}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FashionMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# 定义数据变换\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# 下载并加载训练数据集\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# 下载并加载测试数据集\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# 检查数据加载是否成功\n",
    "data_iter = iter(train_loader)\n",
    "images, labels = next(data_iter)\n",
    "print(f\"Images batch shape: {images.shape}\")\n",
    "print(f\"Labels batch shape: {labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# 定义数据变换\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # 对 RGB 三个通道分别进行归一化\n",
    "])\n",
    "\n",
    "# 下载并加载训练数据集\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# 下载并加载测试数据集\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# 检查数据加载是否成功\n",
    "data_iter = iter(train_loader)\n",
    "images, labels = next(data_iter)\n",
    "print(f\"Images batch shape: {images.shape}\")\n",
    "print(f\"Labels batch shape: {labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# 定义数据变换\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# 下载并加载训练数据集\n",
    "train_dataset = datasets.KMNIST(root='./data', train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# 下载并加载测试数据集\n",
    "test_dataset = datasets.KMNIST(root='./data', train=False, transform=transform, download=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# 检查数据加载是否成功\n",
    "data_iter = iter(train_loader)\n",
    "images, labels = next(data_iter)\n",
    "print(f\"Images batch shape: {images.shape}\")\n",
    "print(f\"Labels batch shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EMNIST Balanced\n",
    "存在一些问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# 定义数据变换\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# 下载并加载训练数据集\n",
    "train_dataset = datasets.EMNIST(root='./data', split='balanced', train=True, transform=transforms.ToTensor(), download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# 下载并加载测试数据集\n",
    "test_dataset = datasets.EMNIST(root='./data', split='balanced', train=False, transform=transforms.ToTensor(), download=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# 检查数据加载是否成功\n",
    "data_iter = iter(train_loader)\n",
    "images, labels = next(data_iter)\n",
    "print(f\"Images batch shape: {images.shape}\")\n",
    "print(f\"Labels batch shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 通用的数据加载器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "def get_dataloader(dataset_name, batch_size=64, root='./data', train=True, selected_classes=None, class_counts=None):\n",
    "    \"\"\"\n",
    "    根据 dataset_name 加载不同的数据集，并应用对应的 Normalize 参数。\n",
    "    \n",
    "    参数:\n",
    "        dataset_name (str): 数据集名称，可选 'MNIST', 'FashionMNIST', 'KMNIST', 'CIFAR10', 'CIFAR100'。\n",
    "        batch_size (int): 数据加载的 batch size。\n",
    "        root (str): 数据集下载或加载的路径。\n",
    "        train (bool): 是否加载训练集，False 表示加载测试集。\n",
    "        selected_classes (list, optional): 需要选择的类别列表，如果为 None，则选择所有类别。\n",
    "        class_counts (list, optional): 每个类别选择的样本数量，和 selected_classes 一一对应。如果为 None，则选择每个类别的全部样本。\n",
    "        \n",
    "    返回:\n",
    "        DataLoader: 配置好的数据加载器。\n",
    "    \"\"\"\n",
    "    # 定义 Normalize 参数\n",
    "    normalize_params = {\n",
    "        'MNIST': ((0.1307,), (0.3081,)),\n",
    "        'FashionMNIST': ((0.2860,), (0.3530,)),\n",
    "        'KMNIST': ((0.1904,), (0.3475,)),\n",
    "        'CIFAR10': ((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        'CIFAR100': ((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
    "    }\n",
    "\n",
    "    # 检查指定的数据集名称是否在支持的列表中\n",
    "    if dataset_name not in normalize_params:\n",
    "        raise ValueError(f\"Unsupported dataset name: {dataset_name}\")\n",
    "\n",
    "    # 获取对应的 Normalize 参数\n",
    "    mean, std = normalize_params[dataset_name]\n",
    "\n",
    "    # 定义数据变换\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "\n",
    "    # 根据 dataset_name 选择数据集\n",
    "    if dataset_name == 'MNIST':\n",
    "        dataset = datasets.MNIST(root=root, train=train, transform=transform, download=True)\n",
    "    elif dataset_name == 'FashionMNIST':\n",
    "        dataset = datasets.FashionMNIST(root=root, train=train, transform=transform, download=True)\n",
    "    elif dataset_name == 'KMNIST':\n",
    "        dataset = datasets.KMNIST(root=root, train=train, transform=transform, download=True)\n",
    "    elif dataset_name == 'CIFAR10':\n",
    "        dataset = datasets.CIFAR10(root=root, train=train, transform=transform, download=True)\n",
    "    elif dataset_name == 'CIFAR100':\n",
    "        dataset = datasets.CIFAR100(root=root, train=train, transform=transform, download=True)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dataset name: {dataset_name}\")\n",
    "\n",
    "    # 筛选指定类别和数量的样本\n",
    "    if selected_classes is not None and class_counts is not None:\n",
    "        # 创建一个空的列表，用于存储符合条件的索引\n",
    "        selected_indices = []\n",
    "        class_counts_dict = {cls: count for cls, count in zip(selected_classes, class_counts)}\n",
    "\n",
    "        # 遍历数据集，筛选出符合条件的样本\n",
    "        for idx, (_, label) in enumerate(dataset):\n",
    "            if label in selected_classes and class_counts_dict[label] > 0:\n",
    "                selected_indices.append(idx)\n",
    "                class_counts_dict[label] -= 1\n",
    "\n",
    "            # 如果每个类别的数据量已经达到要求，跳过\n",
    "            if all(count == 0 for count in class_counts_dict.values()):\n",
    "                break\n",
    "\n",
    "        # 创建一个 Subset，返回选择的样本\n",
    "        dataset = Subset(dataset, selected_indices)\n",
    "\n",
    "    # 如果没有指定类别，且需要筛选每个类别的数量\n",
    "    elif selected_classes is None and class_counts is not None:\n",
    "        # 创建一个空的列表，用于存储符合条件的索引\n",
    "        selected_indices = []\n",
    "        class_counts_dict = {i: count for i, count in zip(range(10), class_counts)}  # 默认为10类（MNIST，CIFAR-10，EMNIST等）\n",
    "\n",
    "        # 遍历数据集，筛选出符合条件的样本\n",
    "        for idx, (_, label) in enumerate(dataset):\n",
    "            if class_counts_dict[label] > 0:\n",
    "                selected_indices.append(idx)\n",
    "                class_counts_dict[label] -= 1\n",
    "\n",
    "            # 如果所有类别的数据量已经达到要求，跳过\n",
    "            if all(count == 0 for count in class_counts_dict.values()):\n",
    "                break\n",
    "\n",
    "        # 创建一个 Subset，返回选择的样本\n",
    "        dataset = Subset(dataset, selected_indices)\n",
    "\n",
    "    # 创建 DataLoader\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=train)\n",
    "    return dataloader\n",
    "\n",
    "selected_classes = [0, 1]\n",
    "class_counts = [100, 100]\n",
    "\n",
    "# 假设我们选择 CIFAR-10 数据集，并且不指定类别，但是限制每种目标的数目\n",
    "#class_counts = [100] * 10\n",
    "\n",
    "train_loader = get_dataloader('CIFAR10', batch_size=64, train=True, selected_classes=selected_classes, class_counts=class_counts)\n",
    "test_loader = get_dataloader('CIFAR10', batch_size=64, train=False)\n",
    "\n",
    "# 检查数据加载是否成功\n",
    "data_iter = iter(train_loader)\n",
    "images, labels = next(data_iter)\n",
    "print(f\"Images batch shape: {images.shape}\")\n",
    "print(f\"Labels batch shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义FBM的单层网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# 定义单层神经网络模型\n",
    "class SingleLayerNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(SingleLayerNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_size)  # 展平图像\n",
    "        out = self.linear(x)\n",
    "        out = (torch.tanh(out) + 1)/2.0\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义Loss函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FBMLoss(nn.Module):\n",
    "    def __init__(self, input_size, lam, df, alpha):\n",
    "        super(FBMLoss, self).__init__()\n",
    "        self.len_out = input_size\n",
    "        self.lam =lam\n",
    "        self.d_f = df\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, inputs, targets, weights):\n",
    "        normal = 0.5 * self.lam / self.len_out * torch.norm(weights, p=2) ** 2\n",
    "        #print(\"normal\", normal)\n",
    "        loss = fast_FermiBose(inputs, targets, self.d_f, self.alpha) + normal\n",
    "        #print(\"diff\", fast_FermiBose(inputs, targets, self.d_f) - FermiBose(inputs, targets, self.d_f))\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 通过向量化操作加速"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_FermiBose(sample, labels, d_f, alpha):\n",
    "    batch, hidden1_features = sample.shape\n",
    "    labels = F.one_hot(labels).float()\n",
    "    # 使用广播计算每对样本之间的 L2 距离的平方和\n",
    "    sample_diff = sample.unsqueeze(1) - sample.unsqueeze(0)  # 扩展维度并相减，得到 (batch, batch, outdim)\n",
    "    D_matrix = torch.sum(sample_diff**2, dim=2)/hidden1_features  # 对最后一个维度求和，得到 (batch, batch) 矩阵\n",
    "\n",
    "    # 计算phi(.)\n",
    "    phi_matrix = F.relu(d_f-D_matrix)\n",
    "\n",
    "    # 计算标签矩阵的乘积，结果是 (batch_size, batch_size)\n",
    "    label_matrix = labels @ labels.T\n",
    "\n",
    "    # 计算bose_loss\n",
    "    bose_loss = torch.mul(D_matrix, label_matrix)\n",
    "\n",
    "    # 计算fermi_loss\n",
    "    fermi_loss = torch.mul(phi_matrix, 1-label_matrix)\n",
    "\n",
    "    # 总loss\n",
    "    loss_matrix = bose_loss + alpha*fermi_loss\n",
    "\n",
    "    # 如果只需要上三角部分（排除重复项和自相似项）\n",
    "    loss_matrix = torch.triu(loss_matrix, diagonal=1)\n",
    "    #cal_count = (loss_matrix > 0.0).sum().item()+1  #统计真正计算的fermi-bose对数量\n",
    "\n",
    "    # 将结果进行 sum（可以选择性地只关注上三角部分的和）\n",
    "    total_loss = loss_matrix.sum()\n",
    "\n",
    "    #return total_loss/cal_count\n",
    "    return 2*total_loss/batch**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 通过循环直观理解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#用循环写这个\n",
    "def FermiBose(sample, labels, d_f, alpha):\n",
    "    batch, hidden1_features = sample.shape\n",
    "    loss = 0\n",
    "    distance = nn.PairwiseDistance(p=2,keepdim=True)\n",
    "    ReLU = nn.ReLU()\n",
    "\n",
    "    cal_count = 1\n",
    "\n",
    "    for i in range(batch):\n",
    "        for j in range(i+1, batch):\n",
    "            D = distance(sample[i], sample[j]) ** 2 / hidden1_features\n",
    "            if labels[i] == labels[j]:\n",
    "                loss += D.sum()\n",
    "                #print(\"bose\", D)\n",
    "                #if D > 0 : cal_count+=1\n",
    "            else:\n",
    "                loss += alpha * ReLU(d_f-D).sum()\n",
    "                #print(\"fermi\", D)\n",
    "                #if ReLU(d_f-D).sum() > 0.0: \n",
    "                #    #print(\"dont, count\", d_f, D, ReLU(d_f-D))\n",
    "                #    cal_count+=1\n",
    "            \n",
    "    #return loss/cal_count\n",
    "    return 2*loss/batch**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练FBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 定义超参数\n",
    "input_size = 28 * 28  # MNIST图像大小是28x28\n",
    "hidden_dim =1000\n",
    "num_classes = 10      # MNIST有10个类别\n",
    "learning_rate = 0.01\n",
    "num_epochs = 20\n",
    "batch_size = 64\n",
    "alpha = 1.0\n",
    "df = 0.02\n",
    "\n",
    "# 实例化模型、定义损失函数和优化器\n",
    "model = SingleLayerNN(input_size, hidden_dim).to(device)\n",
    "criterion = FBMLoss(hidden_dim, 0.01, df, alpha)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model.train()\n",
    "# 训练模型\n",
    "for epoch in range(num_epochs):\n",
    "    for images, labels in train_loader:\n",
    "        # 将图像和标签移动到 GPU 上\n",
    "        images = images.view(-1, 28 * 28).to(device)  # 展平图像并转移到 GPU\n",
    "        labels = labels.to(device)  # 标签移动到 GPU\n",
    "        #labels_one_hot = F.one_hot(labels, num_classes=num_classes).float()\n",
    "        \n",
    "        # 前向传播\n",
    "        outputs = model(images)\n",
    "        #loss = criterion(outputs, labels_one_hot, model.linear.weight)\n",
    "        loss = criterion(outputs, labels, model.linear.weight)\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义最后的全连接层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# 定义单层神经网络模型\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(MLP, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_size)  # 展平图像\n",
    "        out = self.linear(x)\n",
    "        #out = F.softmax(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练全连接层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "model2 = MLP(hidden_dim, num_classes).to(device)\n",
    "criterion2 = nn.CrossEntropyLoss()  # 使用交叉熵损失\n",
    "optimizer = optim.Adam(model2.parameters(), lr=learning_rate)  # 使用随机梯度下降优化器\n",
    "\n",
    "model2.train()\n",
    "# 训练模型\n",
    "epochs = 30\n",
    "for epoch in range(epochs):\n",
    "    for images, labels in train_loader:\n",
    "        # 将图像展平为一维向量，并将标签进行 one-hot 编码\n",
    "        images = images.view(-1, 28 * 28).to(device)  # 展平图像\n",
    "        labels_one_hot = F.one_hot(labels, num_classes=num_classes).float().to(device)  # 将标签转换为 one-hot 编码\n",
    "\n",
    "        # 前向传播\n",
    "        with torch.no_grad():\n",
    "            deal_images = model(images)\n",
    "        outputs = model2(deal_images)\n",
    "\n",
    "        # 计算损失\n",
    "        loss = criterion2(outputs, labels_one_hot)\n",
    "\n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试准确性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 设置模型为评估模式\n",
    "model.eval()\n",
    "model2.eval()\n",
    "\n",
    "# 准确率计数\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# 禁用梯度计算，加速测试过程\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        # 将数据加载到 GPU\n",
    "        images = images.view(-1, 28 * 28).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # 前向传播\n",
    "        outputs = model(images)\n",
    "        outputs = model2(outputs)\n",
    "        \n",
    "        # 获取预测结果\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        # 更新计数\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "# 计算准确率\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy on the test dataset: {accuracy:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "normal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
